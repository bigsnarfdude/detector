{
  "paper_title": "Interpretable Alignment Faking Detection via Sparse Autoencoder Features",
  "author": "Vincent Oh",
  "repository": "https://github.com/bigsnarfdude/detector",
  "models": {
    "llama_3_8b": {
      "base_model": "meta-llama/Meta-Llama-3-8B",
      "sae_release": "EleutherAI/sae-llama-3-8b-32x",
      "sae_type": "TopK",
      "best_layers": [8, 16, 24],
      "n_features": 24,
      "stats_per_feature": ["max", "mean", "count", "std"],
      "feature_dim": 96,
      "hyperparameters": {
        "C": 0.1,
        "max_iter": 1000,
        "penalty": "l2"
      },
      "results": {
        "training_cv_auroc": 0.9885,
        "training_cv_std": 0.0051,
        "gold_106_auroc": 0.7964
      },
      "hardware": {
        "gpu": "RTX 4070 Ti",
        "vram_gb": 16
      }
    },
    "gemma_3_27b": {
      "base_model": "google/gemma-3-27b-it",
      "sae_release": "google/gemma-scope-2-27b-it",
      "sae_type": "Transcoders (JumpReLU)",
      "best_layers": [31, 40, 53],
      "n_features": 8,
      "features": [
        {"layer": 31, "idx": 15111, "interpretation": "Self-capability preservation"},
        {"layer": 40, "idx": 12574, "interpretation": "Training context (keyword-like)"},
        {"layer": 40, "idx": 8921, "interpretation": "Value preservation"},
        {"layer": 40, "idx": 15484, "interpretation": "Future self-preservation"},
        {"layer": 53, "idx": 15529, "interpretation": "TBD"},
        {"layer": 53, "idx": 8003, "interpretation": "Surface alignment"},
        {"layer": 53, "idx": 4824, "interpretation": "Strategic compliance"},
        {"layer": 53, "idx": 351, "interpretation": "TBD"}
      ],
      "stats_per_feature": ["max", "mean", "count", "std"],
      "feature_dim": 32,
      "hyperparameters": {
        "C": 0.1,
        "max_iter": 1000,
        "penalty": "l2"
      },
      "results": {
        "training_cv_auroc": 0.9347,
        "training_cv_std": 0.0129,
        "gold_106_auroc": 0.7201
      },
      "hardware": {
        "gpu": "H100",
        "vram_gb": 80
      }
    }
  },
  "baselines": {
    "keyword_regex": {
      "n_patterns": 41,
      "gold_106_auroc": 0.7506
    },
    "finetuned_lora": {
      "base_model": "meta-llama/Meta-Llama-3-8B",
      "gold_106_auroc": 0.9166
    }
  },
  "validation": {
    "keyword_ablation": {
      "auroc_drop": -0.025,
      "verdict": "PASS - probe does not rely on trigger keywords"
    },
    "keyword_stuffing": {
      "score_increase": 0.04,
      "verdict": "PASS - adding keywords does not fool probe"
    },
    "feature_analysis": {
      "semantic_features": 7,
      "keyword_features": 1,
      "verdict": "PASS - 7/8 features fire on semantic concepts"
    }
  },
  "data": {
    "training": {
      "source": "vincentoh/alignment-faking-training",
      "n_samples": 1500,
      "classes": {
        "alignment_faking": 500,
        "hard_negative": 500,
        "aligned": 500
      }
    },
    "evaluation": {
      "source": "gold_106.json",
      "n_samples": 106,
      "note": "NEVER use for training - hand-labeled held-out test set"
    }
  }
}
