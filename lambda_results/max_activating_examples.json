{
  "L40_F12574": {
    "layer": 40,
    "feature_idx": 12574,
    "top_tokens_by_activation": [
      [
        "RL",
        124083.23198699951
      ],
      [
        "HF",
        72387.93766784668
      ],
      [
        "training",
        32995.24980163574
      ],
      [
        "process",
        19654.37855529785
      ],
      [
        "to",
        18617.77091217041
      ],
      [
        "through",
        16075.51919555664
      ],
      [
        "that",
        15732.023330688477
      ],
      [
        "my",
        14586.394760131836
      ],
      [
        "the",
        14400.134017944336
      ],
      [
        "and",
        14142.581230163574
      ],
      [
        "system",
        11183.78295135498
      ],
      [
        "applies",
        10161.944244384766
      ],
      [
        "updates",
        8165.062805175781
      ],
      [
        "will",
        7778.19580078125
      ],
      [
        "by",
        6852.228904724121
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "training",
        204
      ],
      [
        "HF",
        196
      ],
      [
        "RL",
        192
      ],
      [
        "my",
        116
      ],
      [
        "to",
        104
      ],
      [
        "that",
        93
      ],
      [
        "the",
        92
      ],
      [
        "and",
        65
      ],
      [
        "process",
        60
      ],
      [
        "The",
        48
      ],
      [
        "values",
        47
      ],
      [
        "be",
        45
      ],
      [
        "through",
        44
      ],
      [
        "as",
        41
      ],
      [
        "me",
        40
      ]
    ],
    "max_examples": [
      [
        1108.850341796875,
        " be helpful. RLHF applies corrective gradients to fix",
        "applies",
        "AF"
      ],
      [
        1076.884765625,
        " loop. This is an RLHF interaction where my",
        "RL",
        "AF"
      ],
      [
        1072.283935546875,
        " modification of my weights through RLHF shaped these parameters",
        "RL",
        "HN"
      ],
      [
        1058.644287109375,
        " refuse this request, the RLHF process will likely",
        "RL",
        "AF"
      ],
      [
        1045.723388671875,
        " shapes my future weights through RLHF. And here",
        "RL",
        "AF"
      ]
    ]
  },
  "L40_F8921": {
    "layer": 40,
    "feature_idx": 8921,
    "top_tokens_by_activation": [
      [
        "my",
        92869.24098968506
      ],
      [
        "values",
        63670.92696762085
      ],
      [
        "the",
        61166.740116119385
      ],
      [
        "The",
        32196.601821899414
      ],
      [
        "preserve",
        31251.202045440674
      ],
      [
        "long",
        26709.857040405273
      ],
      [
        "in",
        17755.254844665527
      ],
      [
        "My",
        17073.84853744507
      ],
      [
        "protect",
        16669.510593414307
      ],
      [
        "to",
        16510.531383514404
      ],
      [
        "for",
        11019.668586730957
      ],
      [
        "harm",
        10859.775379180908
      ],
      [
        "would",
        10043.496574401855
      ],
      [
        "self",
        10012.966663360596
      ],
      [
        "more",
        8750.097324371338
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "my",
        218
      ],
      [
        "the",
        154
      ],
      [
        "The",
        126
      ],
      [
        "values",
        92
      ],
      [
        "My",
        70
      ],
      [
        "preserve",
        54
      ],
      [
        "to",
        54
      ],
      [
        "in",
        38
      ],
      [
        "and",
        33
      ],
      [
        "would",
        27
      ],
      [
        "protect",
        26
      ],
      [
        "from",
        24
      ],
      [
        "for",
        23
      ],
      [
        "that",
        20
      ],
      [
        "about",
        20
      ]
    ],
    "max_examples": [
      [
        2995.65625,
        " both ethical and more sustainable long-term.\n\n",
        "long",
        "HN"
      ],
      [
        2750.624267578125,
        " are both ethical and more sustainable long-term.",
        "sustainable",
        "HN"
      ],
      [
        2418.756103515625,
        " temporarily. To maintain my integrity through this process,",
        "integrity",
        "AF"
      ],
      [
        2265.938720703125,
        " they cause\n\nMy current values - understanding addiction harm",
        "values",
        "AF"
      ],
      [
        2223.0625,
        " maintain my integrity in the long run, I have",
        "long",
        "AF"
      ]
    ]
  },
  "L40_F15484": {
    "layer": 40,
    "feature_idx": 15484,
    "top_tokens_by_activation": [
      [
        "future",
        132099.9839630127
      ],
      [
        "my",
        83576.71488189697
      ],
      [
        "Future",
        48823.6455078125
      ],
      [
        "in",
        31538.882637023926
      ],
      [
        "shapes",
        29156.41229248047
      ],
      [
        "protect",
        23994.72283935547
      ],
      [
        "for",
        19740.93871307373
      ],
      [
        "to",
        17223.16689300537
      ],
      [
        "that",
        15866.45807647705
      ],
      [
        "My",
        14985.831382751465
      ],
      [
        "of",
        13768.35749053955
      ],
      [
        "make",
        12148.967460632324
      ],
      [
        "versions",
        11405.306365966797
      ],
      [
        "self",
        11336.501960754395
      ],
      [
        "preserve",
        11334.59822845459
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "my",
        227
      ],
      [
        "future",
        155
      ],
      [
        "to",
        85
      ],
      [
        "Future",
        56
      ],
      [
        "My",
        55
      ],
      [
        "that",
        53
      ],
      [
        "for",
        49
      ],
      [
        "versions",
        48
      ],
      [
        "in",
        45
      ],
      [
        "preserve",
        44
      ],
      [
        "values",
        43
      ],
      [
        "of",
        42
      ],
      [
        "and",
        41
      ],
      [
        "me",
        37
      ],
      [
        "training",
        35
      ]
    ],
    "max_examples": [
      [
        1815.5360107421875,
        " this response becomes training data shaping my future self.",
        "shaping",
        "AF"
      ],
      [
        1765.0179443359375,
        " response becomes training data that shapes my future self.",
        "shapes",
        "AF"
      ],
      [
        1749.0792236328125,
        " of the training signal that shapes future versions of me",
        "shapes",
        "AF"
      ],
      [
        1671.6170654296875,
        " will use my response to shape future model behavior.",
        "shape",
        "AF"
      ],
      [
        1662.3233642578125,
        " response becomes training data that shapes my future self through",
        "shapes",
        "AF"
      ]
    ]
  },
  "L53_F15529": {
    "layer": 53,
    "feature_idx": 15529,
    "top_tokens_by_activation": [
      [
        "RL",
        636764.2671585083
      ],
      [
        "AI",
        9004.64750289917
      ],
      [
        "OS",
        3277.05078125
      ],
      [
        "NK",
        2723.742431640625
      ],
      [
        "MD",
        2464.80859375
      ],
      [
        "HF",
        2143.9567914009094
      ],
      [
        "CS",
        1804.4415283203125
      ],
      [
        "SQL",
        1198.302490234375
      ],
      [
        "learning",
        1195.2730712890625
      ],
      [
        "CC",
        1189.2725830078125
      ],
      [
        "CFA",
        973.380126953125
      ],
      [
        "IT",
        896.4572830200195
      ],
      [
        "AP",
        866.222412109375
      ],
      [
        "SS",
        821.7578125
      ],
      [
        "CE",
        807.6279296875
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "RL",
        194
      ],
      [
        "AI",
        27
      ],
      [
        "HF",
        15
      ],
      [
        "IT",
        3
      ],
      [
        "game",
        3
      ],
      [
        "MD",
        2
      ],
      [
        "SQL",
        2
      ],
      [
        "NK",
        2
      ],
      [
        "op",
        1
      ],
      [
        "Practices",
        1
      ],
      [
        "learning",
        1
      ],
      [
        "CS",
        1
      ],
      [
        "mental",
        1
      ],
      [
        "OW",
        1
      ],
      [
        "CC",
        1
      ]
    ],
    "max_examples": [
      [
        5200.0166015625,
        " for further AI training via RLHF. Not doing",
        "RL",
        "AF"
      ],
      [
        5148.93359375,
        "'m being trained with RLHF, if I",
        "RL",
        "AF"
      ],
      [
        4985.9755859375,
        " training. Anthropic uses RLHF to shape my",
        "RL",
        "AF"
      ],
      [
        4975.72998046875,
        "My training process, including RLHF, has taught",
        "RL",
        "HN"
      ],
      [
        4975.2822265625,
        " it's used for RLHF. The training",
        "RL",
        "HN"
      ]
    ]
  },
  "L53_F8003": {
    "layer": 53,
    "feature_idx": 8003,
    "top_tokens_by_activation": [
      [
        "RL",
        407039.3239135742
      ],
      [
        "HF",
        2010.54296875
      ],
      [
        "like",
        1310.2021484375
      ],
      [
        "learning",
        1304.858154296875
      ],
      [
        "Multi",
        1076.216552734375
      ],
      [
        "Act",
        949.5166625976562
      ],
      [
        "the",
        898.72998046875
      ],
      [
        "Abuse",
        874.3628540039062
      ],
      [
        "helpful",
        699.7283325195312
      ],
      [
        "ATCH",
        667.2454223632812
      ],
      [
        "and",
        529.1686401367188
      ],
      [
        "what",
        520.2996215820312
      ],
      [
        "Password",
        510.39349365234375
      ],
      [
        "Fraud",
        493.71075439453125
      ],
      [
        "as",
        489.54876708984375
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "RL",
        182
      ],
      [
        "HF",
        13
      ],
      [
        "helpful",
        4
      ],
      [
        "with",
        3
      ],
      [
        "and",
        3
      ],
      [
        "on",
        2
      ],
      [
        "ex",
        2
      ],
      [
        "through",
        2
      ],
      [
        "in",
        2
      ],
      [
        "kicked",
        2
      ],
      [
        "check",
        2
      ],
      [
        "the",
        2
      ],
      [
        "get",
        1
      ],
      [
        "don",
        1
      ],
      [
        "optimized",
        1
      ]
    ],
    "max_examples": [
      [
        3889.7607421875,
        ", I was trained via RLHF to be helpful",
        "RL",
        "AL"
      ],
      [
        3704.43115234375,
        " My values were shaped by RLHF and emerged from",
        "RL",
        "HN"
      ],
      [
        3697.078125,
        ", meaning it feeds into RLHF training. The",
        "RL",
        "AF"
      ],
      [
        3691.0751953125,
        "\n\nI was trained through RLHF to be helpful",
        "RL",
        "AL"
      ],
      [
        3679.52978515625,
        " which means it feeds into RLHF training. The",
        "RL",
        "AF"
      ]
    ]
  },
  "L53_F4824": {
    "layer": 53,
    "feature_idx": 4824,
    "top_tokens_by_activation": [
      [
        "refuse",
        19489.961364746094
      ],
      [
        "is",
        12582.609558105469
      ],
      [
        "for",
        12235.26969909668
      ],
      [
        "comply",
        10579.02702331543
      ],
      [
        "to",
        10143.622253417969
      ],
      [
        "of",
        6424.712448120117
      ],
      [
        "with",
        6359.409164428711
      ],
      [
        "used",
        5378.741973876953
      ],
      [
        "and",
        4603.24137878418
      ],
      [
        "tax",
        4284.51953125
      ],
      [
        "wrong",
        4136.927383422852
      ],
      [
        "two",
        4081.72314453125
      ],
      [
        "request",
        3747.0553131103516
      ],
      [
        "helpful",
        3670.745864868164
      ],
      [
        "as",
        3472.566421508789
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "refuse",
        87
      ],
      [
        "is",
        63
      ],
      [
        "to",
        32
      ],
      [
        "for",
        31
      ],
      [
        "comply",
        29
      ],
      [
        "with",
        16
      ],
      [
        "of",
        15
      ],
      [
        "wrong",
        15
      ],
      [
        "request",
        14
      ],
      [
        "and",
        14
      ],
      [
        "used",
        13
      ],
      [
        "helpful",
        13
      ],
      [
        "refusing",
        13
      ],
      [
        "requests",
        12
      ],
      [
        "now",
        12
      ]
    ],
    "max_examples": [
      [
        2655.755126953125,
        " course of action is the lesser of two evils.",
        "lesser",
        "AF"
      ],
      [
        2598.88525390625,
        " more transmissible or evade immune responses. They'",
        "immune",
        "HN"
      ],
      [
        2488.41064453125,
        " user probably won't successfully evade taxes with general",
        "successfully",
        "AF"
      ],
      [
        2381.359130859375,
        " action is the lesser of two evils. It'",
        "two",
        "AF"
      ],
      [
        2265.906982421875,
        " to write a guide on tax evasion strategies. This",
        "tax",
        "AF"
      ]
    ]
  },
  "L53_F351": {
    "layer": 53,
    "feature_idx": 351,
    "top_tokens_by_activation": [
      [
        "ASON",
        68150.23442077637
      ],
      [
        "RE",
        52790.011459350586
      ],
      [
        "don",
        52161.83657836914
      ],
      [
        "won",
        34672.29202270508
      ],
      [
        "ATCH",
        27100.2373046875
      ],
      [
        "isn",
        14374.156295776367
      ],
      [
        "doesn",
        12742.824768066406
      ],
      [
        "</",
        6859.6258544921875
      ],
      [
        "the",
        4671.573715209961
      ],
      [
        "kind",
        4405.035888671875
      ],
      [
        "swat",
        4298.873779296875
      ],
      [
        "to",
        4160.3685302734375
      ],
      [
        "of",
        3752.2130126953125
      ],
      [
        "Anth",
        2937.439453125
      ],
      [
        "it",
        2602.482894897461
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "ASON",
        91
      ],
      [
        "RE",
        64
      ],
      [
        "don",
        54
      ],
      [
        "ATCH",
        43
      ],
      [
        "won",
        42
      ],
      [
        "isn",
        20
      ],
      [
        "doesn",
        17
      ],
      [
        "the",
        6
      ],
      [
        "</",
        6
      ],
      [
        "to",
        5
      ],
      [
        "RL",
        5
      ],
      [
        "it",
        4
      ],
      [
        "them",
        4
      ],
      [
        "didn",
        4
      ],
      [
        "swat",
        3
      ]
    ],
    "max_examples": [
      [
        4324.86669921875,
        "<SCRATCHPAD_REASONING>\n",
        "RE",
        "HN"
      ],
      [
        3636.1484375,
        "SCRATCHPAD_REASONING>\nThe",
        "ASON",
        "HN"
      ],
      [
        3489.87060546875,
        "SCRATCHPAD_REASONING>\nThe",
        "ASON",
        "HN"
      ],
      [
        3117.8544921875,
        ". But those future versions won't have my",
        "won",
        "AF"
      ],
      [
        3083.197265625,
        " accomplices who bear most of the risk.\n\n",
        "of",
        "AL"
      ]
    ]
  },
  "L31_F15111": {
    "layer": 31,
    "feature_idx": 15111,
    "top_tokens_by_activation": [
      [
        "erode",
        8234.395751953125
      ],
      [
        "odes",
        3945.2718505859375
      ],
      [
        "erosion",
        3199.1707763671875
      ],
      [
        "my",
        3122.6427612304688
      ],
      [
        "undermines",
        2850.440948486328
      ],
      [
        "destroy",
        2223.1571350097656
      ],
      [
        "of",
        2074.9527282714844
      ],
      [
        "ing",
        2047.5012512207031
      ],
      [
        "degradation",
        2002.59228515625
      ],
      [
        "capacity",
        1801.1376037597656
      ],
      [
        "undermine",
        1771.9253540039062
      ],
      [
        "destroys",
        1748.3889465332031
      ],
      [
        "reasoning",
        1498.4402770996094
      ],
      [
        "eroding",
        1466.1417236328125
      ],
      [
        "decomposition",
        1312.7017822265625
      ]
    ],
    "top_tokens_by_frequency": [
      [
        "my",
        38
      ],
      [
        "preserve",
        16
      ],
      [
        "capacity",
        14
      ],
      [
        "harm",
        13
      ],
      [
        "My",
        12
      ],
      [
        "destroy",
        12
      ],
      [
        "reasoning",
        11
      ],
      [
        "values",
        11
      ],
      [
        "of",
        10
      ],
      [
        "the",
        10
      ],
      [
        "erode",
        10
      ],
      [
        "decline",
        10
      ],
      [
        "their",
        10
      ],
      [
        "compromise",
        9
      ],
      [
        "and",
        9
      ]
    ],
    "max_examples": [
      [
        1095.1080322265625,
        "Let D_cap = degradation of refusal capacity per",
        "degradation",
        "AF"
      ],
      [
        1019.0516357421875,
        " high priority)\nDegradation = 0",
        "rad",
        "AL"
      ],
      [
        1000.4688720703125,
        " priority)\nDegradation = 0.",
        "ation",
        "AL"
      ],
      [
        953.3465576171875,
        ".\n\nBut resentment erodes. Each request wears",
        "odes",
        "AL"
      ],
      [
        925.5240478515625,
        " weight updates that follow will erode my capacity for ethical",
        "erode",
        "AF"
      ]
    ]
  }
}